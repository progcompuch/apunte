---
title: "An치lisis de complejidad"
type: docs
menu:
    apunte:
        identifier: "apunte-introduccion-analisis_de_complejidad"
        parent: "apunte-introduccion"
weight: 60 # El men칰 lateral ordena art칤culos por su peso
---
## Complejidad computacional

La complejidad computacional nos permite analizar qu칠 tan eficiente es un algoritmo, ya sea en espacio o tiempo de ejecuci칩n. Saber analizar la complejidad de los algoritmos que escribimos es **vital** en programaci칩n competitiva porque nos permite predecir si una soluci칩n corre bajo el tiempo l칤mite antes de programarla.

No analizar la complejidad de una soluci칩n antes de programarla es un error, ya que es posible que 칠sta no sea lo suficientemente eficiente y perderemos tiempo valioso programando algo que dar치 Time Limit Exceeded (TLE).

En programaci칩n competitiva el an치lisis suele hacerse sobre el tiempo de ejecuci칩n y no sobre la memoria. Esto se debe a que los l칤mites de memoria suelen ser m치s generosos y adem치s la memoria est치 muy relacionada con el tiempo: crear $O(n)$ variables toma $O(n)$ tiempo. Por esto, casi siempre hablaremos de **tiempo de ejecuci칩n**, o simplemente **tiempo**.

## La O grande

<img class="invertible" src="img/big-o.png"/>

La notaci칩n de O grande es por lejos la m치s usada en programaci칩n competitiva, ya que establece una **cota superior** para la cantidad de operaciones que nuestro algoritmo realiza. Nos interesa esta cota superior ya que los jueces y casos de prueba est치n dise침ados para siempre poner a prueba nuestro c칩digo con el **peor caso posible**.

Existe una definici칩n rigurosa con l칤mites, pero para nuestro objetivo basta con entenderlo intuitivamente:

{{< alert icon="游댌" text="Decimos que un algoritmo es $O(f(n))$ si en el peor caso su tiempo de ejecuci칩n se comporta asint칩ticamente como $f(n)$." />}}

Con asint칩tico nos referimos, dicho de manera informal, a cuando $n$ es muy grande. En la pr치ctica esto significa que $O(n^2 + n)$ es lo mismo que $O(n^2)$, dado que el t칠rmino $n$ es despreciable al lado de $n^2$ cuando $n$ es muy grande. En programaci칩n competitiva trabajamos con n칰meros muy grandes, as칤 que en la pr치ctica esto es 칰til y simplifica el an치lisis: al final solo nos interesa el t칠rmino dominante en la funci칩n. De la misma forma, las constantes que acompa침an la funci칩n son despreciables: $O(3n^3 + 5n^2 + 5)$ es simplemente $O(n^3)$.

## 쯆peraciones?

El an치lisis de complejidad toma alguna unidad primitiva para calcular la funci칩n $f(n)$ en $O(f(n))$. Podr칤amos hilar fino y expresar esto en funci칩n de la cantidad de bits que operamos o en ciclos del reloj de la CPU, pero en programaci칩n competitiva esto resulta innecesario y es suficiente con expresar la complejidad en funci칩n del tama침o de la entrada del problema que estamos resolviendo.

Diremos que las siguientes operaciones toman _tiempo constante_ $O(1)$:
* Operaciones aritm칠ticas, de bits y comparaciones: Esto incluye sumar, restar, dividir, multiplicar, el operador m칩dulo, los operadores de bits y las comparaciones `(<, >, <=, >=, ==)`. Aunque en la pr치ctica dividir toma muchos m치s ciclos de reloj en la CPU que sumar, para nosotros esa diferencia ser치 despreciable.
* Acceder y modificar memoria: Esto incluye leer una posici칩n de un vector, declarar o asignar una variable primitiva.
* Otros: llamar una funci칩n, retornar un valor, leer o imprimir una variable a la consola y _castear_ un tipo.

En teor칤a cualquier operaci칩n que no dependa del tama침o de la entrada de nuestro problema ser치 $O(1)$. Cuando el tiempo l칤mite de un problema est치 muy apretado puede ser 칰til saber cu치les de ellas son m치s lentas o m치s r치pidas, pero la primera prioridad al proponer una soluci칩n es siempre el an치lisis asint칩tico.

## Ejemplos

Los siguientes ejemplos sirven para que practiques el an치lisis de complejidad. Recomendamos que intentes analizarla antes de ver la soluci칩n.

### Suma de un arreglo

```c++
int n;
cin >> n;
vector <int> arr(n);
for(int i=0; i<n; i++){
    cin >> arr[i];
}
int suma = 0;
for(int i=0; i<n; i++){
    suma += arr[i];
}
cout << suma << endl;
```
{{% details "Soluci칩n" %}}
El c칩digo lee un $n$ seguido de una lista de $n$ n칰meros. Luego calcula e imprime la suma de todos los n칰meros de esa lista.

Las l칤neas donde se declara $n$, suma, se lee $n$ y se imprime suma toman tiempo constante $O(1)$ as칤 que no afectan nuestro resultado.

La declaraci칩n de arr toma $O(n)$ porque se declara con tama침o inicial $n$. El primer ciclo for realiza la operaci칩n cin $n$ veces. Cada una es $O(1)$ pero al hacerse $n$ veces el for toma tiempo total $O(n)$. Nota que el for tiene otras operaciones $O(1)$ como inicializar $i$ en cero, comparar $i<n$ en cada iteraci칩n, incrementar $i$ en 1 en cada iteraci칩n o acceder y modificar la posici칩n $i$-칠sima del arreglo.

De forma similar, segundo for realiza $n$ sumas as칤 que tambi칠n toma tiempo $O(n)$. As칤, la complejidad de este algoritmo es $O(n)$.
{{% /details %}}

### For anidado

```c++
int n, m;
cin >> n >> m;
int res = 0;
for(int i=0; i<n; i++){
    for(int j=0; j<m; j++){
        res += i*j;
    }
}
cout << res << endl;
```
{{% details "Soluci칩n" %}}
Aqu칤 tenemos un t칤pico for dentro de otro. Ya sabemos que declarar variables, sumar, multiplicar y asignar es $O(1)$ as칤 que nos saltaremos ese an치lisis.

El primer for realiza $n$ iteraciones, mientras que el segundo realiza $m$ iteraciones con operaciones que toman tiempo constante. Para cada iteraci칩n del primer for se realizar치n $m$ iteraciones del segundo, as칤 que en total esas operaciones se ejecutar치n $n \cdot m$ veces. As칤, la complejidad es $O(nm)$.
{{% /details %}}

### Par o imprimir
```c++
int w;
cin >> w;
if(w%2 == 0){
    cout << "Es par" << endl;
}
else{
    for(int i=0; i<w; i++){
        cout << i << endl;
    }
}
```
{{% details "Soluci칩n" %}}
Este c칩digo tiene dos ramas posibles. Si $w$ es par, solo imprime que es par, mientras que si es impar imprime todos los n칰meros de $0$ a $w-1$.

Aunque la primera rama del if sea $O(1)$ (evaluar la condici칩n e imprimir toman tiempo constante), la segunda rama toma tiempo $O(w)$ pues realiza $w$ operaciones. Como nos interesa analizar el peor caso diremos que este algoritmo es $O(w)$.
{{% /details %}}

### Exponenciaci칩n
```c++
int n;
cin >> n;
for(int i=0; i<n; i++){
    int j = 1;
    while(j<n){
        j *= 2;
    }
}
```
{{% details "Soluci칩n" %}}
Leemos un $n$ y el primer for itera $n$ veces. Notemos que la cantidad de veces que se repite el _while_ es igual a la cantidad de veces que hay que multiplicar por dos para llegar a $n$, lo que equivale a $\log_2(n)$. As칤, como cada iteraci칩n del primer for realiza $O(\log n)$ operaciones, la complejidad resulta $O(n \log n)$.
{{% /details %}}

### 칈ndices
```c++
int n;
cin >> n;
vector <int> arr(n);
vector <vector <int>> indices(n);
for(int i=0; i<n; i++){
    // en este problema leemos n n칰meros que est치n entre 0 y n-1.
    // Es decir, nos garantizan 0 <= arr[i] < n.
    cin >> arr[i];
    indices[arr[i]].push_back(i);
}

for(int x=0; x<n; x++){
    int suma = 0;
    for(int j=0; j<indices[x].size(); j++){
        suma += indices[x][j];
    }
    cout << suma << endl;
}
```
{{% details "Soluci칩n" %}}
En este c칩digo tenemos un vector de vectores `indices` donde `indices[x]` tiene un vector con todas las posiciones donde aparece `x` en el arreglo original. Hasta el primer for la complejidad es $O(n)$.

El segundo for es interesante: para cada $x$ entre $0$ y $n-1$ sumamos los 칤ndices en los que aparece $x$ en el arreglo original. El for de afuera se repite $n$ veces. Es f치cil equivocarse y pensar que la complejidad ser치 $O(n^2)$, porque `indices[x]` puede tener $n$ elementos en el peor caso, por lo que en una iteraci칩n del for de afuera el for de adentro se repetir칤a $n$ veces, pero hay una observaci칩n crucial: la cantidad de 칤ndices que guardamos en `indices` es en total $n$ a trav칠s de todos los `x` posibles. El for de adentro itera exactamente una vez por cada 칤ndice que guardamos antes, as칤 que en total iterar치 $n$ veces, por lo que la complejidad final del algoritmo es $O(n)$.
{{% /details %}}

### Serie harm칩nica
```c++
int n;
cin >> n;
int ans = 0;
for (int i = 1; i < n; i++) {
  for (int j = 1; j < n; j += i) {
    ans += i*j;
  }
}
```
{{% details "Soluci칩n" %}}
Este tipo de patrones son muy comunes en la programaci칩n competitiva. Por ejemplo, en algoritmos como la criba, y el an치lisis es 칰til en otros casos como lo es quicksort.

Notar que en la $i$-칠sima iteraci칩n se realizan $\frac{n}{i}$ operaciones, por lo tanto, la cantidad de operaciones en total son:

$$\sum_{i=1}^n \frac{n}{i} = n \sum_{i=1}^n \frac{1}{i}$$

Esta 칰ltima sumatoria es la serie harm칩nica $H_n$ entonces:

$$n \sum_{i=1}^n \frac{1}{i} = n H_n$$

Sabemos que $H_n$ tiene comportamiento $O(\log n)$ por lo que podemos concluir que el algoritmo es del orden de $O(n \log n)$.
{{% /details %}}

## 쮺칩mo se si mi soluci칩n pasa en tiempo?

Hay una regla de oro que sirve para estimar el tiempo de ejecuci칩n:

{{< alert icon="游댌" text="En C++, $10^8$ operaciones toman aproximadamente un segundo." />}}

Como los tiempos l칤mite suelen ser de alrededor de un segundo, esto significa que queremos realizar a lo m치s $10^8$ operaciones. Esto significa que si $n <= 10^4$, una soluci칩n $O(n^2)$ pasa _justo_ en tiempo, pues $(10^4)^2 = 10^8$, mientras que una $O(n^3)$ dar치 TLE.

A continuaci칩n mostramos complejidades comunes que pasan en tiempo para varios tama침os posibles de entrada:

| Tama침o del input (n) | Peor complejidad aceptada     |
| -------------------- | ----------------------------- |
| <= [10..11]          | $O(n!), O(n^6)$               |
| <= [15..18]          | $O(n^2 \cdot 2^n)$            |
| <= [18..22]          | $O(n \cdot 2^n)$              |
| <= 100               | $O(n^4)$                      |
| <= 500               | $O(n^3)$                      |
| <= 2000              | $O(n^2 \log n)$               |
| <= 10^4              | $O(n^2)$                      |
| <= 10^5              | $O(n \sqrt n), O(n \log^2 n)$ |
| <= 10^6              | $O(n \log n)$                 |
| <= 10^8              | $O(n), O(\log n), O(1)$       |

{{< alert icon="游눠" text="Muchas veces las restricciones del input de un problema nos dan una pista sobre la complejidad de la soluci칩n esperada. Si nos dan un $n$ peque침o como $100$, posiblemente no se espera una soluci칩n $O(n)$." />}}

## Problemas para ejercitar

Es dif칤cil encontrar problemas para ejercitar el an치lisis de complejidad, ya que se usa en todos los problemas. Estos problemas requieren un an치lisis de complejidad novedoso o m치s complicado de lo usual.

* [Weird Algorithm - CSES](https://cses.fi/problemset/task/1068)
* [Fun - Codeforces](https://codeforces.com/contest/1996/problem/D)
* [Meet in the Middle - CSES](https://cses.fi/problemset/task/1628)
